#@ load("@ytt:data", "data")
#@ load("@ytt:base64", "base64")

#@ start_bosh_b64 = base64.encode(data.values.start_bosh_script)
#@ warden_start_bosh_b64 = base64.encode(data.values.warden_start_bosh_script)

---
resources:
  - name: bosh-docker-cpi-image
    type: registry-image
    icon: docker
    source:
      repository: bosh/docker-cpi

  - name: warden-cpi-repo
    type: git
    icon: github
    source:
      uri: https://github.com/rkoster/noble-concourse-nested-cpi-validation.git
      branch: main

  - name: bosh-deployment-repo
    type: git
    icon: github
    source:
      uri: https://github.com/cloudfoundry/bosh-deployment.git
      branch: master

  - name: zookeeper-release
    type: git
    icon: github
    source:
      uri: https://github.com/cppforlife/zookeeper-release
      branch: master

  - name: oci-build-task
    type: registry-image
    icon: docker
    source:
      repository: concourse/oci-build-task
      tag: "0.14.8"

  - name: warden-cpi-containerd-image
    type: registry-image
    icon: docker
    source:
      repository: ((docker_registry_host))/warden-cpi-containerd
      tag: latest
      username: admin
      password: ((docker_registry_password))
      insecure: true

  - name: warden-cpi-runc-image
    type: registry-image
    icon: docker
    source:
      repository: ((docker_registry_host))/warden-cpi-runc
      tag: latest
      username: admin
      password: ((docker_registry_password))
      insecure: true

  - name: upstream-warden-cpi-image
    type: registry-image
    icon: docker
    source:
      repository: ((docker_registry_host))/upstream-warden-cpi
      tag: latest
      username: admin
      password: ((docker_registry_password))
      insecure: true

  - name: bosh-repo
    type: git
    icon: github
    source:
      uri: https://github.com/cloudfoundry/bosh.git
      branch: noble-cut-over
      paths:
        - ci/dockerfiles/warden-cpi
        - ci/dockerfiles/integration
        - src/.ruby-version

  - name: ubuntu-image
    type: registry-image
    icon: docker
    source:
      repository: ubuntu
      tag: noble

  - name: integration-image
    type: registry-image
    icon: docker
    source:
      repository: ((docker_registry_host))/integration
      tag: latest
      username: admin
      password: ((docker_registry_password))
      insecure: true

jobs:

  - name: build-integration-image
    plan:
      - in_parallel:
          - get: bosh-repo
            trigger: false
          - get: bosh-deployment-repo
            trigger: false
          - get: ubuntu-image
          - get: oci-build-task
      - task: build-docker-args
        image: ubuntu-image
        config:
          platform: linux
          inputs:
            - name: bosh-repo
            - name: bosh-deployment-repo
          outputs:
            - name: docker-build-args
          run:
            path: bash
            args:
              - -exc
              - |
                #! Build docker args for integration image
                #! Simplified version of upstream build-docker-args.sh
                
                export DEBIAN_FRONTEND=noninteractive
                apt-get update -y
                apt-get install -y --no-install-recommends ca-certificates curl jq
                
                # Fetch latest release URLs from GitHub
                bosh_cli_url="$(curl -s https://api.github.com/repos/cloudfoundry/bosh-cli/releases/latest \
                  | jq -r '.assets[] | select(.name | contains ("linux-amd64")) | .browser_download_url')"
                
                meta4_cli_url="$(curl -s https://api.github.com/repos/dpb587/metalink/releases/latest \
                  | jq -r '.assets[] | select(.name | match("meta4-[0-9]+.[0-9]+.[0-9]+-linux-amd64")) | .browser_download_url')"
                
                yq_cli_url="$(curl -s https://api.github.com/repos/mikefarah/yq/releases/latest \
                  | jq -r '.assets[] | select(.name | endswith ("linux_amd64")) | .browser_download_url')"
                
                ruby_install_url="$(curl -s https://api.github.com/repos/postmodern/ruby-install/releases/latest \
                  | jq -r '.assets[] | select(.name | endswith ("tar.gz")) | .browser_download_url')"
                
                golangci_lint_install_url="$(curl -s https://api.github.com/repos/golangci/golangci-lint/releases/latest \
                  | jq -r '.assets[] | select(.name | match("golangci-lint-[0-9]+.[0-9]+.[0-9]+-linux-amd64.tar.gz")) | .browser_download_url')"
                
                # Get UAA release URL from bosh-deployment
                apt-get install -y --no-install-recommends python3-pip python3-venv
                python3 -m venv /tmp/yq-venv
                /tmp/yq-venv/bin/pip install yq
                uaa_release_url="$(/tmp/yq-venv/bin/yq -r '.[] | select(.release == "uaa").value.url' < bosh-deployment-repo/uaa.yml)"
                
                ruby_version="$(cat bosh-repo/src/.ruby-version)"
                
                cat << JSON > docker-build-args/docker-build-args.json
                {
                  "BOSH_CLI_URL": "${bosh_cli_url}",
                  "META4_CLI_URL": "${meta4_cli_url}",
                  "GOLANGCI_LINT_INSTALL_URL": "${golangci_lint_install_url}",
                  "YQ_CLI_URL": "${yq_cli_url}",
                  "RUBY_INSTALL_URL": "${ruby_install_url}",
                  "RUBY_VERSION": "${ruby_version}",
                  "GEM_HOME": "/usr/local/bundle",
                  "UAA_RELEASE_URL": "${uaa_release_url}",
                  "JAVA_INSTALL_PREFIX": "/usr/lib/jvm",
                  "POSTGRES_MAJOR_VERSION": "15"
                }
                JSON
                
                echo "=== Docker build args ==="
                cat docker-build-args/docker-build-args.json
      - task: prepare-build-context
        image: ubuntu-image
        config:
          platform: linux
          inputs:
            - name: bosh-repo
            - name: docker-build-args
          outputs:
            - name: build-context
          run:
            path: bash
            args:
              - -exc
              - |
                cp bosh-repo/ci/dockerfiles/integration/Dockerfile build-context/
                cp docker-build-args/docker-build-args.json build-context/
                ls -la build-context/
      - task: build-image
        privileged: true
        image: oci-build-task
        timeout: 2h
        config:
          platform: linux
          inputs:
            - name: build-context
          outputs:
            - name: image
          params:
            CONTEXT: build-context
            BUILD_ARG_BASE_IMAGE: "ubuntu:noble"
            BUILD_ARG_DEBIAN_FRONTEND: "noninteractive"
            BUILD_ARGS_FILE: build-context/docker-build-args.json
            BUILDKIT_PROGRESS: plain
          run:
            path: sh
            args:
              - -c
              - |
                echo "=== Starting integration image build ==="
                echo "This may take 20-30 minutes on first build..."
                echo ""
                mount -t cgroup2 none /sys/fs/cgroup 2>/dev/null || true
                exec build
      - put: integration-image
        params:
          image: image/image.tar
        get_params:
          skip_download: true

  - name: deploy-zookeeper-on-docker-bosh
    plan:
      - in_parallel:
          - get: bosh-docker-cpi-image
          - get: zookeeper-release
            trigger: false

      - task: start-bosh-and-deploy-zookeeper
        image: bosh-docker-cpi-image
        privileged: true
        config:
          platform: linux
          inputs:
            - name: zookeeper-release
          run:
            path: bash
            args:
              - -c
              - #@ "set -euo pipefail\n\necho \"=== Checking Disk Space ===\"\ndf -h /scratch\n\necho \"=== Cleaning up previous builds ===\"\nrm -rf /scratch/docker /tmp/local-bosh /tmp/tmp.* /root/.bosh || true\ndf -h /scratch\n\necho \"=== Creating patched start-bosh script from base64 ===\"\ncp /usr/local/bin/start-bosh /usr/local/bin/start-bosh.original\n\n# Decode base64-encoded patched script\nbase64 -d > /usr/local/bin/start-bosh <<'BASE64SCRIPT'\n" + start_bosh_b64 + "\nBASE64SCRIPT\n\nchmod +x /usr/local/bin/start-bosh\n\necho \"=== Starting BOSH Director with Docker CPI (patched version) ===\"\nstart-bosh\n\n# Source the environment variables\nsource /tmp/local-bosh/director/env\n\necho \"=== BOSH Director Environment ===\"\necho \"BOSH_ENVIRONMENT: ${BOSH_ENVIRONMENT}\"\necho \"BOSH_CLIENT: ${BOSH_CLIENT}\"\n\necho \"=== Verifying BOSH Director ===\"\nbosh env\n\necho \"=== Cloud Config ===\"\nbosh cloud-config\n\necho \"=== Creating and Uploading Zookeeper Release ===\"\ncd zookeeper-release\nbosh create-release --force --timestamp-version\nbosh upload-release\ncd ..\n\necho \"=== Uploading Stemcell ===\"\nbosh upload-stemcell --sha1 86662513fbbb9200aca8f0c24fa69e822f49e18c \\\n  https://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-jammy-go_agent?v=1.631\n\necho \"=== Creating ops-file for single instance ===\"\ncat > /tmp/single-instance.yml <<'OPSFILE'\n---\n# Ops-file to deploy a single zookeeper instance instead of 5\n- type: replace\n  path: /instance_groups/name=zookeeper/instances\n  value: 1\n\n- type: replace\n  path: /instance_groups/name=zookeeper/azs\n  value: [z1]\n\n# Remove smoke-tests errand for simplicity\n- type: remove\n  path: /instance_groups/name=smoke-tests\n\n# Update stemcell to jammy\n- type: replace\n  path: /stemcells/alias=default/os\n  value: ubuntu-jammy\nOPSFILE\n\necho \"=== Deploying Zookeeper (Single Instance) ===\"\nbosh -n deploy \\\n  -d zookeeper \\\n  zookeeper-release/manifests/zookeeper.yml \\\n  -o /tmp/single-instance.yml\n\necho \"=== Deployment Status ===\"\nbosh -d zookeeper instances\n\necho \"=== Zookeeper VMs ===\"\nbosh -d zookeeper vms\n\necho \"=== Testing Zookeeper ===\"\nbosh -d zookeeper ssh zookeeper/0 -c \"\n  set -e\n  echo '=== Zookeeper Process Status ==='\n  sudo /var/vcap/bosh/bin/monit summary\n  \n  echo '=== Zookeeper Server Status ==='\n  echo stat | nc localhost 2181 || echo 'Could not connect to Zookeeper'\n\"\n\necho \"=== Cleaning Up ===\"\nbosh -d zookeeper -n delete-deployment\n\necho \"=== Pipeline Complete - Docker CPI SUCCESS ===\"\n"

  - name: build-warden-cpi-containerd-image
    plan:
      - in_parallel:
          - get: warden-cpi-repo
            trigger: false
          - get: bosh-deployment-repo
            trigger: false
          - get: oci-build-task

      - task: prepare-build-context
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: ubuntu
              tag: latest
          inputs:
            - name: warden-cpi-repo
            - name: bosh-deployment-repo
          outputs:
            - name: build-context
          run:
            path: bash
            args:
              - -c
              - |
                set -eu
                echo "=== Preparing build context ==="
                
                # Copy warden-cpi-containerd directory contents
                cp -r warden-cpi-repo/warden-cpi-containerd/* build-context/
                
                # Copy bosh-deployment into build context
                cp -r bosh-deployment-repo build-context/bosh-deployment
                
                echo "=== Build context prepared ==="
                ls -la build-context/
                ls -la build-context/bosh-deployment/ | head -20

      - task: build-image
        privileged: true
        image: oci-build-task
        timeout: 30m
        config:
          platform: linux
          inputs:
            - name: build-context
          outputs:
            - name: image
          params:
            CONTEXT: build-context
            BUILD_ARG_BASE_IMAGE: "ubuntu:noble"
            BUILDKIT_PROGRESS: plain
          run:
            path: sh
            args:
              - -c
              - |
                echo "=== Building warden-cpi-containerd image ==="
                # Mount cgroup2 to enable oci-build-task cgroup v2 detection
                # Guardian/gdn mounts a tmpfs at /sys/fs/cgroup but doesn't expose cgroup v2
                # This allows the setup-cgroups script to detect cgroup v2 and skip v1 setup
                mount -t cgroup2 none /sys/fs/cgroup 2>/dev/null || true
                exec build

      - put: warden-cpi-containerd-image
        params:
          image: image/image.tar
        get_params:
          skip_download: true

  - name: deploy-zookeeper-on-warden-containerd
    plan:
      - in_parallel:
          - get: warden-cpi-containerd-image
            trigger: true
            passed: [build-warden-cpi-containerd-image]
          - get: zookeeper-release
            trigger: false

      - task: deploy-with-warden-cpi
        image: warden-cpi-containerd-image
        privileged: true
        config:
          platform: linux
          inputs:
            - name: zookeeper-release
          run:
            path: bash
            args:
              - -c
              - |
                set -euxo pipefail
                
                echo "=== Starting containerd daemon ==="
                mkdir -p /run/containerd
                mkdir -p /var/vcap/sys/run/containerd
                containerd &
                
                # Wait for containerd to be ready
                for i in $(seq 1 30); do
                  if ctr version >/dev/null 2>&1; then
                    echo "containerd is ready"
                    # Create symlink so Garden can find containerd socket
                    ln -sf /run/containerd/containerd.sock /var/vcap/sys/run/containerd/containerd.sock
                    echo "Created symlink for containerd socket"
                    break
                  fi
                  echo "Waiting for containerd... ($i/30)"
                  sleep 1
                done
                
                echo "=== Starting Garden and BOSH Director with Warden CPI (containerd backend) ==="
                
                # Start Garden manually with better error handling
                /var/vcap/jobs/garden/bin/pre-start
                /var/vcap/jobs/garden/bin/garden_ctl start &
                
                # Give Garden time to start
                sleep 5
                
                # Manually check if Garden API is responding
                echo "=== Checking if Garden API is responding ===" 
                for i in $(seq 1 30); do
                  if curl -f http://127.0.0.1:7777/ping 2>/dev/null; then
                    echo "Garden is responding!"
                    break
                  fi
                  echo "Attempt $i: Garden not ready yet..."
                  
                  # Check if garden process is still running
                  if ! pgrep -f garden > /dev/null; then
                    echo "ERROR: Garden process has died!"
                    echo "=== Garden logs ==="
                    ls -la /var/vcap/sys/log/garden/ || true
                    echo "=== garden stdout ==="
                    cat /var/vcap/sys/log/garden/garden.stdout.log 2>/dev/null || echo "No stdout log"
                    echo "=== garden stderr ==="
                    cat /var/vcap/sys/log/garden/garden.stderr.log 2>/dev/null || echo "No stderr log"
                    echo "=== containerd stdout ==="
                    cat /var/vcap/sys/log/garden/containerd.stdout.log 2>/dev/null || echo "No containerd stdout log"
                    echo "=== containerd stderr ==="
                    cat /var/vcap/sys/log/garden/containerd.stderr.log 2>/dev/null || echo "No containerd stderr log"
                    echo "=== Sleeping for 10 minutes for debugging ==="
                    sleep 600
                    exit 1
                  fi
                  
                  sleep 2
                done
                
                # If we get here and Garden still isn't responding, check logs
                if ! curl -f http://127.0.0.1:7777/ping 2>/dev/null; then
                  echo "ERROR: Garden never became ready after 30 attempts!"
                  echo "=== Garden process status ==="
                  ps aux | grep -E 'garden|containerd' || true
                  echo "=== Garden logs ==="
                  ls -la /var/vcap/sys/log/garden/ || true
                  echo "=== garden stdout ==="
                  cat /var/vcap/sys/log/garden/garden.stdout.log 2>/dev/null || echo "No stdout log"
                  echo "=== garden stderr ==="
                  cat /var/vcap/sys/log/garden/garden.stderr.log 2>/dev/null || echo "No stderr log"
                  echo "=== containerd stdout ==="
                  cat /var/vcap/sys/log/garden/containerd.stdout.log 2>/dev/null || echo "No containerd stdout log"
                  echo "=== containerd stderr ==="
                  cat /var/vcap/sys/log/garden/containerd.stderr.log 2>/dev/null || echo "No containerd stderr log"
                  echo "=== Network status ==="
                  netstat -tlnp || ss -tlnp || true
                  echo "=== Sleeping for 10 minutes for debugging ==="
                  sleep 600
                  exit 1
                fi
                
                # Continue with BOSH director deployment
                echo "=== Deploying BOSH Director ==="
                cd /usr/local/bosh-deployment
                export BOSH_DIRECTOR_IP="192.168.56.6"
                local_bosh_dir="/tmp/local-bosh/director"
                mkdir -p ${local_bosh_dir}
                
                bosh int bosh.yml \
                  -o bosh-lite.yml \
                  -o warden/cpi.yml \
                  -o uaa.yml \
                  -o credhub.yml \
                  -o jumpbox-user.yml \
                  -o /usr/local/releases/local-releases.yml \
                  -v director_name=bosh-lite \
                  -v internal_ip=${BOSH_DIRECTOR_IP} \
                  -v internal_gw=192.168.56.1 \
                  -v internal_cidr=192.168.56.0/24 \
                  -v outbound_network_name=NatNetwork \
                  -v garden_host=127.0.0.1 > "${local_bosh_dir}/bosh-director.yml"
                
                echo "=== Checking for CPI binary dependencies ==="
                # Extract and check the CPI binary
                echo "--- Extracting warden CPI release ---"
                mkdir -p /tmp/warden-cpi-inspect
                tar -xzf /usr/local/releases/warden-cpi.tgz -C /tmp/warden-cpi-inspect
                
                if [ -f /tmp/warden-cpi-inspect/jobs/warden_cpi.tgz ]; then
                  mkdir -p /tmp/warden-cpi-job
                  tar -xzf /tmp/warden-cpi-inspect/jobs/warden_cpi.tgz -C /tmp/warden-cpi-job
                  
                  echo "--- Checking job bin/cpi template ---"
                  if [ -f /tmp/warden-cpi-job/templates/cpi.erb ]; then
                    echo "CPI template exists"
                    head -20 /tmp/warden-cpi-job/templates/cpi.erb
                  fi
                fi
                
                echo "=== Running bosh create-env with DEBUG logging ==="
                BOSH_LOG_LEVEL=debug bosh create-env "${local_bosh_dir}/bosh-director.yml" \
                     --vars-store="${local_bosh_dir}/creds.yml" \
                     --state="${local_bosh_dir}/state.json" 2>&1 | tee /tmp/bosh-create-env.log || {
                  echo "=== BOSH create-env failed, analyzing installation ==="
                  
                  echo "--- Checking CPI installation ---"
                  CPI_PKG_PATH=$(find /root/.bosh -path "*/packages/warden_cpi/bin/cpi" -type f 2>/dev/null | head -1)
                  if [ -n "$CPI_PKG_PATH" ]; then
                    echo "Found CPI package binary at: $CPI_PKG_PATH"
                    ls -lh "$CPI_PKG_PATH"
                    file "$CPI_PKG_PATH" || echo "file command not available"
                    
                    echo "--- Checking CPI binary dependencies (ldd) ---"
                    ldd "$CPI_PKG_PATH" || echo "ldd check failed - missing dynamic libraries?"
                    
                    echo "--- Attempting to run CPI directly ---"
                    "$CPI_PKG_PATH" || echo "Direct execution failed with exit code: $?"
                  fi
                  
                  echo "--- Checking job wrapper script ---"
                  JOB_CPI_PATH=$(find /root/.bosh -path "*/jobs/warden_cpi/bin/cpi" -type f 2>/dev/null | head -1)
                  if [ -n "$JOB_CPI_PATH" ]; then
                    echo "Found job CPI wrapper at: $JOB_CPI_PATH"
                    ls -lh "$JOB_CPI_PATH"
                    echo "--- CPI wrapper script content (first 30 lines) ---"
                    head -30 "$JOB_CPI_PATH"
                  fi
                  
                  echo "--- Last 100 lines of bosh create-env debug log ---"
                  tail -100 /tmp/bosh-create-env.log
                  
                  exit 1
                }
                
                bosh int "${local_bosh_dir}/creds.yml" --path /director_ssl/ca > "${local_bosh_dir}/ca.crt"
                
                cat <<EOF > "${local_bosh_dir}/env"
                export BOSH_ENVIRONMENT="${BOSH_DIRECTOR_IP}"
                export BOSH_CLIENT=admin
                export BOSH_CLIENT_SECRET=`bosh int "${local_bosh_dir}/creds.yml" --path /admin_password`
                export BOSH_CA_CERT="${local_bosh_dir}/ca.crt"
                EOF
                
                source "${local_bosh_dir}/env"
                bosh -n update-cloud-config warden/cloud-config.yml
                ip route add 10.244.0.0/15 via ${BOSH_DIRECTOR_IP}
                
                # Source the environment variables
                source /tmp/local-bosh/director/env
                
                echo "=== BOSH Director Environment ==="
                echo "BOSH_ENVIRONMENT: ${BOSH_ENVIRONMENT}"
                echo "BOSH_CLIENT: ${BOSH_CLIENT}"
                
                echo "=== Verifying BOSH Director ==="
                bosh env
                
                echo "=== Cloud Config ==="
                bosh cloud-config
                
                echo "=== Creating and Uploading Zookeeper Release ==="
                cd /workspace/zookeeper-release
                bosh create-release --force --timestamp-version
                bosh upload-release
                cd /
                
                echo "=== Uploading Stemcell ==="
                bosh upload-stemcell --sha1 8fbcaa623769ec29c39f4c8679337a8c95e8e4a8 \
                  https://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-jammy-go_agent?v=1.632
                
                echo "=== Creating ops-file for single instance ==="
                cat > /tmp/single-instance.yml <<'OPSFILE'
                ---
                - type: replace
                  path: /instance_groups/name=zookeeper/instances
                  value: 1
                
                - type: replace
                  path: /instance_groups/name=zookeeper/azs
                  value: [z1]
                
                - type: remove
                  path: /instance_groups/name=smoke-tests
                
                - type: replace
                  path: /stemcells/alias=default/os
                  value: ubuntu-jammy
                OPSFILE
                
                echo "=== Deploying Zookeeper (Single Instance) ==="
                bosh -n deploy \
                  -d zookeeper \
                  /workspace/zookeeper-release/manifests/zookeeper.yml \
                  -o /tmp/single-instance.yml
                
                echo "=== Deployment Status ==="
                bosh -d zookeeper instances
                
                echo "=== Zookeeper VMs ==="
                bosh -d zookeeper vms
                
                echo "=== Testing Zookeeper ==="
                bosh -d zookeeper ssh zookeeper/0 -c "
                  set -e
                  echo '=== Zookeeper Process Status ==='
                  sudo /var/vcap/bosh/bin/monit summary
                  
                  echo '=== Zookeeper Server Status ==='
                  echo stat | nc localhost 2181 || echo 'Could not connect to Zookeeper'
                "
                
                echo "=== Cleaning Up ==="
                bosh -d zookeeper -n delete-deployment
                
                echo "=== Pipeline Complete - Warden CPI with Containerd SUCCESS ==="

  - name: build-warden-cpi-runc-image
    plan:
      - in_parallel:
          - get: warden-cpi-repo
            trigger: false
          - get: bosh-deployment-repo
            trigger: false
          - get: oci-build-task
      - task: prepare-build-context
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: alpine
          inputs:
            - name: warden-cpi-repo
            - name: bosh-deployment-repo
          outputs:
            - name: build-context
          run:
            path: sh
            args:
              - -exc
              - |
                # Copy runc-based warden-cpi configuration
                cp -r warden-cpi-repo/warden-cpi-runc/* build-context/
                
                # Copy necessary ops files
                mkdir -p build-context/ops-files
                cp warden-cpi-repo/ops-files/zookeeper-single-instance.yml build-context/ops-files/
                cp warden-cpi-repo/ops-files/docker-registry.yml build-context/ops-files/
                
                # Copy bosh-deployment manifests
                mkdir -p build-context/bosh-deployment
                cp -r bosh-deployment-repo/bosh.yml build-context/bosh-deployment/
                cp -r bosh-deployment-repo/bosh-lite.yml build-context/bosh-deployment/
                cp -r bosh-deployment-repo/docker build-context/bosh-deployment/
                cp -r bosh-deployment-repo/warden build-context/bosh-deployment/
                cp -r bosh-deployment-repo/uaa.yml build-context/bosh-deployment/
                cp -r bosh-deployment-repo/credhub.yml build-context/bosh-deployment/
                cp -r bosh-deployment-repo/jumpbox-user.yml build-context/bosh-deployment/
                
                # Show the install-garden-runc.rb configuration (GrootFS enabled by default)
                echo "=== Garden configuration ==="
                cat build-context/install-garden-runc.rb
                
                echo "Build context prepared for runc runtime with GrootFS enabled"
                ls -la build-context/
      - task: build-image
        privileged: true
        image: oci-build-task
        timeout: 30m
        config:
          platform: linux
          inputs:
            - name: build-context
          outputs:
            - name: image
          params:
            CONTEXT: build-context
            BUILD_ARG_BASE_IMAGE: "ubuntu:noble"
            BUILDKIT_PROGRESS: plain
          run:
            path: sh
            args:
              - -c
              - |
                echo "=== Building warden-cpi-runc image ==="
                # Mount cgroup2 to enable oci-build-task cgroup v2 detection
                # Guardian/gdn mounts a tmpfs at /sys/fs/cgroup but doesn't expose cgroup v2
                # This allows the setup-cgroups script to detect cgroup v2 and skip v1 setup
                mount -t cgroup2 none /sys/fs/cgroup 2>/dev/null || true
                exec build
      - put: warden-cpi-runc-image
        params:
          image: image/image.tar

  - name: deploy-zookeeper-on-warden-runc
    plan:
      - in_parallel:
          - get: warden-cpi-runc-image
            trigger: true
            passed: [build-warden-cpi-runc-image]
          - get: zookeeper-release
      - task: deploy-with-warden-cpi
        privileged: true
        image: warden-cpi-runc-image
        config:
          platform: linux
          inputs:
            - name: zookeeper-release
          run:
            path: bash
            args:
              - -c
              - #@ "set -euo pipefail\n\necho \"=== Starting Warden CPI Runc Test with GrootFS ===\"\necho \"Runtime: Garden with runc runtime\"\necho \"Image Plugin: GrootFS (enabled)\"\necho \"Base OS: Ubuntu Noble 24.04\"\n\n# Mount cgroup filesystem for Garden\necho \"=== Setting up cgroup v2 ===\"\nmkdir -p /sys/fs/cgroup\nmount -t cgroup2 none /sys/fs/cgroup || echo \"cgroup2 already mounted\"\n\n# Patch start-bosh script with latest version\necho \"=== Patching start-bosh script ===\"\nbase64 -d > /usr/local/bin/start-bosh <<'BASE64SCRIPT'\n" + warden_start_bosh_b64 + "\nBASE64SCRIPT\nchmod +x /usr/local/bin/start-bosh\n\n# Start BOSH director with warden-cpi (this also starts Garden)\necho \"=== Starting BOSH Director ===\"\n/usr/local/bin/start-bosh\n\n# Source BOSH environment created by start-bosh\nsource /tmp/local-bosh/director/env\n\necho \"=== BOSH Environment ===\"\nbosh env\n\n# Upload stemcell\necho \"=== Uploading Warden Stemcell ===\"\nbosh upload-stemcell --sha1 86662513fbbb9200aca8f0c24fa69e822f49e18c \\\n  https://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-jammy-go_agent?v=1.631\n\n# Upload zookeeper release\necho \"=== Uploading Zookeeper Release ===\"\ncd zookeeper-release\nbosh create-release --force --timestamp-version\nbosh upload-release\ncd ..\n\n# Deploy zookeeper\necho \"=== Deploying Zookeeper ===\"\ncat > /tmp/single-instance.yml <<'OPSFILE'\n---\n- type: replace\n  path: /instance_groups/name=zookeeper/instances\n  value: 1\n- type: replace\n  path: /instance_groups/name=zookeeper/azs\n  value: [z1]\n- type: remove\n  path: /instance_groups/name=smoke-tests\n- type: replace\n  path: /stemcells/alias=default/os\n  value: ubuntu-jammy\nOPSFILE\n\nbosh -n deploy \\\n  -d zookeeper \\\n  zookeeper-release/manifests/zookeeper.yml \\\n  -o /tmp/single-instance.yml\n\necho \"=== Zookeeper Deployment Status ===\"\nbosh -d zookeeper instances --details\nbosh -d zookeeper vms --vitals\n\n# Test zookeeper\necho \"=== Testing Zookeeper ===\"\nbosh -d zookeeper ssh zookeeper/0 -c \"\n  set -e\n  echo 'Testing Zookeeper connectivity'\n  echo '=== Zookeeper Server Status ==='\n  echo stat | nc localhost 2181 || echo 'Could not connect to Zookeeper'\n\"\n\necho \"=== Cleaning Up ===\"\nbosh -d zookeeper -n delete-deployment\n\necho \"=== Pipeline Complete - Warden CPI with runc SUCCESS ===\"\n"

  #! Upstream warden-cpi image build job - uses pure upstream Dockerfile from noble-cut-over
  - name: build-upstream-warden-cpi-image
    plan:
      - in_parallel:
          - get: bosh-repo
            trigger: false
          - get: bosh-deployment-repo
            trigger: false
          - get: integration-image
            trigger: true
            passed: [build-integration-image]
          - get: oci-build-task
      - task: prepare-build-context
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: alpine
          inputs:
            - name: bosh-repo
            - name: bosh-deployment-repo
          outputs:
            - name: build-context
          run:
            path: sh
            args:
              - -exc
              - |
                #! Use pure upstream warden-cpi from noble-cut-over branch
                #! Base image: our locally-built integration image
                
                # Copy all upstream warden-cpi files
                cp bosh-repo/ci/dockerfiles/warden-cpi/Dockerfile build-context/
                cp bosh-repo/ci/dockerfiles/warden-cpi/install-garden.rb build-context/
                cp bosh-repo/ci/dockerfiles/warden-cpi/template-renderer.rb build-context/
                cp bosh-repo/ci/dockerfiles/warden-cpi/local-releases.yml build-context/
                cp bosh-repo/ci/dockerfiles/warden-cpi/start-bosh.sh build-context/
                
                # Copy bosh-deployment
                cp -r bosh-deployment-repo build-context/bosh-deployment
                
                echo "=== Build context (pure upstream noble-cut-over) ==="
                ls -la build-context/
                echo "=== Dockerfile ==="
                cat build-context/Dockerfile
                echo "=== start-bosh.sh (first 30 lines) ==="
                head -30 build-context/start-bosh.sh
      - task: build-image
        privileged: true
        image: oci-build-task
        timeout: 30m
        config:
          platform: linux
          inputs:
            - name: build-context
            - name: integration-image
          outputs:
            - name: image
          params:
            CONTEXT: build-context
            IMAGE_ARG_BASE_IMAGE: integration-image/image.tar
            BUILDKIT_PROGRESS: plain
          run:
            path: sh
            args:
              - -c
              - |
                echo "=== Building upstream warden-cpi image ==="
                mount -t cgroup2 none /sys/fs/cgroup 2>/dev/null || true
                exec build
      - put: upstream-warden-cpi-image
        params:
          image: image/image.tar
        get_params:
          skip_download: true

  #! Deploy zookeeper using unmodified upstream warden-cpi image for comparison
  - name: deploy-zookeeper-on-upstream-warden
    plan:
      - in_parallel:
          - get: upstream-warden-cpi-image
            trigger: true
            passed: [build-upstream-warden-cpi-image]
          - get: zookeeper-release
      - task: deploy-with-upstream-warden-cpi
        privileged: true
        image: upstream-warden-cpi-image
        config:
          platform: linux
          inputs:
            - name: zookeeper-release
          run:
            path: bash
            args:
              - -c
              - |
                set -euo pipefail
                
                echo "=== Upstream Warden CPI Test (Unmodified) ==="
                echo "This uses the exact upstream warden-cpi image from cloudfoundry/bosh"
                echo "Base image: bosh/main-bosh-docker"
                
                # Mount cgroup filesystem for Garden
                echo "=== Setting up cgroup v2 ==="
                mkdir -p /sys/fs/cgroup
                mount -t cgroup2 none /sys/fs/cgroup || echo "cgroup2 already mounted"
                
                # Use unmodified upstream start-bosh script
                echo "=== Starting BOSH Director with upstream start-bosh ==="
                /usr/local/bin/start-bosh
                
                # Source BOSH environment
                source /tmp/local-bosh/director/env
                
                echo "=== BOSH Environment ==="
                bosh env
                
                # Upload stemcell
                echo "=== Uploading Warden Stemcell ==="
                bosh upload-stemcell --sha1 86662513fbbb9200aca8f0c24fa69e822f49e18c \
                  https://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-jammy-go_agent?v=1.631
                
                # Upload zookeeper release
                echo "=== Uploading Zookeeper Release ==="
                cd zookeeper-release
                bosh create-release --force --timestamp-version
                bosh upload-release
                cd ..
                
                # Deploy zookeeper
                echo "=== Deploying Zookeeper ==="
                cat > /tmp/single-instance.yml <<'OPSFILE'
                ---
                - type: replace
                  path: /instance_groups/name=zookeeper/instances
                  value: 1
                - type: replace
                  path: /instance_groups/name=zookeeper/azs
                  value: [z1]
                - type: remove
                  path: /instance_groups/name=smoke-tests
                - type: replace
                  path: /stemcells/alias=default/os
                  value: ubuntu-jammy
                OPSFILE
                
                bosh -n deploy \
                  -d zookeeper \
                  zookeeper-release/manifests/zookeeper.yml \
                  -o /tmp/single-instance.yml
                
                echo "=== Zookeeper Deployment Status ==="
                bosh -d zookeeper instances --details
                bosh -d zookeeper vms --vitals
                
                echo "=== Cleaning Up ==="
                bosh -d zookeeper -n delete-deployment
                
                echo "=== Pipeline Complete - Upstream Warden CPI SUCCESS ==="
